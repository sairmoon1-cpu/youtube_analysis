import streamlit as st
from googleapiclient.discovery import build
import pandas as pd
from collections import Counter
from soynlp.tokenizer import RegexTokenizer
import re
import altair as alt

# âœ… ìƒ˜í”Œ
SAMPLE_URL = "https://www.youtube.com/watch?v=WXuK6gekU1Y"
API_KEY = st.secrets["youtube_api_key"]

# ğŸ” video ID ì¶”ì¶œ
def extract_video_id(url):
    pattern = r"(?:v=|youtu\.be/)([\w-]+)"
    match = re.search(pattern, url)
    return match.group(1) if match else None

# âœ… ì˜ìƒ ì—…ë¡œë“œ ì‹œê° ìˆ˜ì§‘
def get_video_published_time(video_id, api_key):
    youtube = build("youtube", "v3", developerKey=api_key)
    response = youtube.videos().list(part="snippet", id=video_id).execute()
    return response["items"][0]["snippet"]["publishedAt"]

# ğŸ’¬ ëŒ“ê¸€ ìˆ˜ì§‘ (ë‚´ìš©, ì‹œê°„, ì¢‹ì•„ìš” ìˆ˜ í¬í•¨)
def get_comments(video_id, api_key, max_comments=100):
    youtube = build("youtube", "v3", developerKey=api_key)
    comments, timestamps, likes = [], [], []
    next_page_token = None

    while True:
        response = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=100,
            pageToken=next_page_token,
            order="relevance",
            textFormat="plainText"
        ).execute()

        for item in response["items"]:
            snippet = item["snippet"]["topLevelComment"]["snippet"]
            comments.append(snippet["textDisplay"])
            timestamps.append(snippet["publishedAt"])
            likes.append(snippet.get("likeCount", 0))

        next_page_token = response.get("nextPageToken")
        if not next_page_token or (max_comments != -1 and len(comments) >= max_comments):
            break

    return (
        comments[:max_comments] if max_comments != -1 else comments,
        timestamps[:max_comments] if max_comments != -1 else timestamps,
        likes[:max_comments] if max_comments != -1 else likes
    )

# ----------------- Streamlit ì•± ------------------

st.title("ğŸ“Š YouTube ëŒ“ê¸€ ì‹œê°„ & ì¢‹ì•„ìš” ìˆ˜ ë¶„ì„ê¸°")

youtube_url = st.text_input("ğŸ“º ì˜ìƒ URL", value=SAMPLE_URL)

col1, col2 = st.columns(2)
with col1:
    select_count = st.selectbox("ëŒ“ê¸€ ê°œìˆ˜ (ë¹ ë¥¸ ì„ íƒ)", ["100", "500", "1000", "ëª¨ë‘"], index=0)
with col2:
    slider_count = st.slider("ëŒ“ê¸€ ê°œìˆ˜ (ì„¸ë¶€ ì¡°ì ˆ)", 100, 1000, step=100, value=100)

comment_limit = -1 if select_count == "ëª¨ë‘" else max(int(select_count), slider_count)

if st.button("ëŒ“ê¸€ ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œì‘"):
    video_id = extract_video_id(youtube_url)
    if not video_id:
        st.error("âŒ ìœ íš¨í•œ YouTube URLì´ ì•„ë‹™ë‹ˆë‹¤.")
        st.stop()

    with st.spinner("ğŸ“… ì˜ìƒ ì—…ë¡œë“œ ì‹œê° í™•ì¸ ì¤‘..."):
        video_published_at = get_video_published_time(video_id, API_KEY)
        video_uploaded_time = pd.to_datetime(video_published_at)

    with st.spinner("ğŸ’¬ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘..."):
        comments, timestamps, likes = get_comments(video_id, API_KEY, comment_limit)

    if not comments:
        st.warning("ğŸ˜¥ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

    df = pd.DataFrame({
        "ëŒ“ê¸€ ë‚´ìš©": comments,
        "ì‘ì„± ì‹œê°": pd.to_datetime(timestamps),
        "ì¢‹ì•„ìš” ìˆ˜": likes
    })

    # ì—…ë¡œë“œ ê¸°ì¤€ ì‹œê°„ ê²½ê³¼ ê³„ì‚°
    df["ê²½ê³¼ ì‹œê°„ (ë¶„)"] = (df["ì‘ì„± ì‹œê°"] - video_uploaded_time).dt.total_seconds() / 60
    df["ê²½ê³¼ ì‹œê°„ (ì‹œ)"] = df["ê²½ê³¼ ì‹œê°„ (ë¶„)"] / 60
    df["ì‘ì„± ì‹œê°„ëŒ€ (ì‹œ)"] = df["ì‘ì„± ì‹œê°"].dt.hour

    st.success(f"âœ… ëŒ“ê¸€ {len(df)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ! ì˜ìƒ ì—…ë¡œë“œ ì‹œê°: {video_uploaded_time}")

    # ---------------- ì‹œê°í™” ----------------

    st.subheader("ğŸ“ˆ ì˜ìƒ ì—…ë¡œë“œ ì´í›„ ëŒ“ê¸€ ìˆ˜ ì¶”ì´")

    df_time = df.copy()
    df_time["ê²½ê³¼ ì‹œê°„ (ì‹œ)"] = df_time["ê²½ê³¼ ì‹œê°„ (ì‹œ)"].round().astype(int)
    hourly_count = df_time.groupby("ê²½ê³¼ ì‹œê°„ (ì‹œ)").size().reset_index(name="ëŒ“ê¸€ ìˆ˜")

    chart1 = alt.Chart(hourly_count).mark_line(point=True).encode(
        x=alt.X("ê²½ê³¼ ì‹œê°„ (ì‹œ):Q", title="ì—…ë¡œë“œ ì´í›„ ê²½ê³¼ ì‹œê°„ (ì‹œê°„ ë‹¨ìœ„)"),
        y=alt.Y("ëŒ“ê¸€ ìˆ˜:Q"),
        tooltip=["ê²½ê³¼ ì‹œê°„ (ì‹œ)", "ëŒ“ê¸€ ìˆ˜"]
    ).properties(
        title="ì—…ë¡œë“œ í›„ ì‹œê°„ë³„ ëŒ“ê¸€ ìˆ˜ ì¶”ì´"
    )

    st.altair_chart(chart1, use_container_width=True)

    st.subheader("ğŸŸ¢ ëŒ“ê¸€ ì‘ì„± ì‹œê° vs ì¢‹ì•„ìš” ìˆ˜")

    chart2 = alt.Chart(df).mark_circle(size=60).encode(
        x=alt.X("ì‘ì„± ì‹œê°:T", title="ëŒ“ê¸€ ì‘ì„± ì‹œê°„"),
        y=alt.Y("ì¢‹ì•„ìš” ìˆ˜:Q"),
        tooltip=["ëŒ“ê¸€ ë‚´ìš©", "ì¢‹ì•„ìš” ìˆ˜", "ì‘ì„± ì‹œê°"]
    ).interactive().properties(title="ëŒ“ê¸€ ì‹œê°„ê³¼ ì¢‹ì•„ìš” ìˆ˜ ê´€ê³„")

    st.altair_chart(chart2, use_container_width=True)

    st.subheader("ğŸ•’ ëŒ“ê¸€ ì‹œê°„ëŒ€ë³„ ì¢‹ì•„ìš” ìˆ˜")

    hourly_likes = df.groupby("ì‘ì„± ì‹œê°„ëŒ€ (ì‹œ)")["ì¢‹ì•„ìš” ìˆ˜"].sum().reset_index()

    chart3 = alt.Chart(hourly_likes).mark_bar().encode(
        x=alt.X("ì‘ì„± ì‹œê°„ëŒ€ (ì‹œ):O", title="ëŒ“ê¸€ ì‘ì„± ì‹œê°„ëŒ€ (24ì‹œê°„)"),
        y=alt.Y("ì¢‹ì•„ìš” ìˆ˜:Q"),
        tooltip=["ì‘ì„± ì‹œê°„ëŒ€ (ì‹œ)", "ì¢‹ì•„ìš” ìˆ˜"]
    ).properties(title="ì‹œê°„ëŒ€ë³„ ì¢‹ì•„ìš” ìˆ˜ í•©ê³„")

    st.altair_chart(chart3, use_container_width=True)
