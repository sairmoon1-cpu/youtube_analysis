import streamlit as st
from googleapiclient.discovery import build
import pandas as pd
from collections import Counter
import altair as alt
import re
from datetime import datetime, timedelta

# âœ… ìƒ˜í”Œ
SAMPLE_URL = "https://www.youtube.com/watch?v=WXuK6gekU1Y"
API_KEY = st.secrets["youtube_api_key"]

# ì˜ìƒ ID ì¶”ì¶œ
def extract_video_id(url):
    pattern = r"(?:v=|youtu\.be/)([\w-]+)"
    match = re.search(pattern, url)
    return match.group(1) if match else None

# ì˜ìƒ ì—…ë¡œë“œì¼ ìˆ˜ì§‘
def get_video_upload_time(video_id, api_key):
    youtube = build("youtube", "v3", developerKey=api_key)
    response = youtube.videos().list(part="snippet", id=video_id).execute()
    upload_time = response["items"][0]["snippet"]["publishedAt"]
    return pd.to_datetime(upload_time)

# ëŒ“ê¸€ ìˆ˜ì§‘ (ì‘ì„± ì‹œê° + ì¢‹ì•„ìš” í¬í•¨)
def get_comments(video_id, api_key, max_comments=100):
    youtube = build("youtube", "v3", developerKey=api_key)
    comments, timestamps, likes = [], [], []
    next_page_token = None

    while True:
        response = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=100,
            pageToken=next_page_token,
            order="relevance",
            textFormat="plainText"
        ).execute()

        for item in response["items"]:
            snippet = item["snippet"]["topLevelComment"]["snippet"]
            comments.append(snippet["textDisplay"])
            timestamps.append(snippet["publishedAt"])
            likes.append(snippet.get("likeCount", 0))

        next_page_token = response.get("nextPageToken")
        if not next_page_token or (max_comments != -1 and len(comments) >= max_comments):
            break

    return (
        comments[:max_comments] if max_comments != -1 else comments,
        pd.to_datetime(timestamps[:max_comments] if max_comments != -1 else timestamps),
        likes[:max_comments] if max_comments != -1 else likes
    )

# ------------------- Streamlit ì•± -------------------

st.title("â° YouTube ëŒ“ê¸€ ì‹œê°„ ë¶„ì„ê¸°")

youtube_url = st.text_input("ğŸ“º YouTube ì˜ìƒ URL", value=SAMPLE_URL)
col1, col2 = st.columns(2)
with col1:
    select_count = st.selectbox("ëŒ“ê¸€ ìˆ˜ (ë¹ ë¥¸ ì„ íƒ)", ["100", "500", "1000", "ëª¨ë‘"], index=0)
with col2:
    slider_count = st.slider("ëŒ“ê¸€ ìˆ˜ (ì„¸ë¶€ ì¡°ì ˆ)", 100, 1000, step=100, value=100)

limit = -1 if select_count == "ëª¨ë‘" else max(int(select_count), slider_count)

if st.button("ë¶„ì„ ì‹œì‘"):
    video_id = extract_video_id(youtube_url)
    if not video_id:
        st.error("âš ï¸ ìœ íš¨í•œ YouTube URLì´ ì•„ë‹™ë‹ˆë‹¤.")
        st.stop()

    with st.spinner("ğŸ“¥ ì˜ìƒ ì—…ë¡œë“œì¼ ì¡°íšŒ ì¤‘..."):
        upload_time = get_video_upload_time(video_id, API_KEY)

    with st.spinner("ğŸ’¬ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘..."):
        comments, timestamps, likes = get_comments(video_id, API_KEY, limit)

    if not comments:
        st.warning("ëŒ“ê¸€ì„ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    # DataFrame êµ¬ì„±
    df = pd.DataFrame({
        "ëŒ“ê¸€ ë‚´ìš©": comments,
        "ì‘ì„± ì‹œê°": timestamps,
        "ì¢‹ì•„ìš” ìˆ˜": likes
    })
    df["ê²½ê³¼ ì‹œê°„ (ì‹œ)"] = ((df["ì‘ì„± ì‹œê°"] - upload_time).dt.total_seconds() // 3600).astype(int)
    df["ì‹œê°„ëŒ€ (ì‹œ)"] = df["ì‘ì„± ì‹œê°"].dt.hour

    # --------------------- ğŸ“ˆ 1. ëˆ„ì  ëŒ“ê¸€ ìˆ˜ ê·¸ë˜í”„ ---------------------
    st.subheader("ğŸ“ˆ ì—…ë¡œë“œ ì´í›„ ëŒ“ê¸€ ëˆ„ì  ìˆ˜")

    hourly_counts = df.groupby("ê²½ê³¼ ì‹œê°„ (ì‹œ)").size().reset_index(name="ëŒ“ê¸€ ìˆ˜")
    hourly_counts["ëˆ„ì  ëŒ“ê¸€ ìˆ˜"] = hourly_counts["ëŒ“ê¸€ ìˆ˜"].cumsum()

    # 1ì£¼ì¼ ì´ë‚´ ìµœëŒ€ ì¦ê°€êµ¬ê°„ ê°•ì¡°
    within_week = hourly_counts[hourly_counts["ê²½ê³¼ ì‹œê°„ (ì‹œ)"] <= 168]
    diffs = within_week["ëˆ„ì  ëŒ“ê¸€ ìˆ˜"].diff().fillna(0)
    max_idx = diffs.idxmax()
    highlight_hour = within_week.loc[max_idx, "ê²½ê³¼ ì‹œê°„ (ì‹œ)"]

    base_line = alt.Chart(hourly_counts).mark_line().encode(
        x="ê²½ê³¼ ì‹œê°„ (ì‹œ):Q",
        y="ëˆ„ì  ëŒ“ê¸€ ìˆ˜:Q",
        tooltip=["ê²½ê³¼ ì‹œê°„ (ì‹œ)", "ëˆ„ì  ëŒ“ê¸€ ìˆ˜"]
    )

    highlight_point = alt.Chart(hourly_counts[hourly_counts["ê²½ê³¼ ì‹œê°„ (ì‹œ)"] == highlight_hour]).mark_point(
        color="red", size=100
    ).encode(
        x="ê²½ê³¼ ì‹œê°„ (ì‹œ):Q",
        y="ëˆ„ì  ëŒ“ê¸€ ìˆ˜:Q"
    )

    st.altair_chart(base_line + highlight_point, use_container_width=True)

    # ------------------- â± 2. ëŒ“ê¸€ ì‘ì„± ì‹œê° vs ì¢‹ì•„ìš” ìˆ˜ -------------------
    st.subheader("ğŸ§­ ëŒ“ê¸€ ì‘ì„± ì‹œê° vs ì¢‹ì•„ìš” ìˆ˜")

    st.altair_chart(
        alt.Chart(df).mark_circle(size=60, opacity=0.6).encode(
            x="ì‘ì„± ì‹œê°:T",
            y="ì¢‹ì•„ìš” ìˆ˜:Q",
            tooltip=["ëŒ“ê¸€ ë‚´ìš©", "ì¢‹ì•„ìš” ìˆ˜", "ì‘ì„± ì‹œê°"]
        ).interactive(),
        use_container_width=True
    )

    # ---------------- ğŸ•° 3. ëŒ“ê¸€ ì‹œê°„ëŒ€ë³„ ì¢‹ì•„ìš” ìˆ˜ í•©ê³„ ----------------
    st.subheader("ğŸ•’ ëŒ“ê¸€ ì‹œê°„ëŒ€ë³„ ì¢‹ì•„ìš” ìˆ˜ í•©ê³„")

    hourly_likes = df.groupby("ì‹œê°„ëŒ€ (ì‹œ)")["ì¢‹ì•„ìš” ìˆ˜"].sum().reset_index()

    st.altair_chart(
        alt.Chart(hourly_likes).mark_bar().encode(
            x=alt.X("ì‹œê°„ëŒ€ (ì‹œ):O", sort="ascending"),
            y="ì¢‹ì•„ìš” ìˆ˜:Q",
            tooltip=["ì‹œê°„ëŒ€ (ì‹œ)", "ì¢‹ì•„ìš” ìˆ˜"]
        ).properties(
            width=600,
            height=400
        ),
        use_container_width=True
    )
