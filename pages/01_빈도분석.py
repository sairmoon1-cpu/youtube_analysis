import streamlit as st
from googleapiclient.discovery import build
import pandas as pd
from collections import Counter
from soynlp.tokenizer import RegexTokenizer
import re
import altair as alt

# âœ… ìƒ˜í”Œ URL
SAMPLE_URL = "https://www.youtube.com/watch?v=WXuK6gekU1Y"
API_KEY = st.secrets["youtube_api_key"]

# ğŸ¯ video ID ì¶”ì¶œ
def extract_video_id(url):
    pattern = r"(?:v=|youtu\.be/)([\w-]+)"
    match = re.search(pattern, url)
    return match.group(1) if match else None

# ğŸ’¬ ëŒ“ê¸€ ìˆ˜ì§‘ í•¨ìˆ˜ (ë‚´ìš©ë§Œ)
def get_comments(video_id, api_key, max_comments=100):
    youtube = build("youtube", "v3", developerKey=api_key)
    comments = []
    next_page_token = None

    while True:
        response = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=100,
            pageToken=next_page_token,
            order="relevance",
            textFormat="plainText"
        ).execute()

        for item in response["items"]:
            snippet = item["snippet"]["topLevelComment"]["snippet"]
            comments.append(snippet["textDisplay"])

        next_page_token = response.get("nextPageToken")
        if not next_page_token or (max_comments != -1 and len(comments) >= max_comments):
            break

    return comments[:max_comments] if max_comments != -1 else comments

# ğŸ§  ëª…ì‚¬ ì¤‘ì‹¬ í† í° ì¶”ì¶œ
@st.cache_data
def extract_nouns(comments):
    tokenizer = RegexTokenizer()
    tokens = []
    for comment in comments:
        tokens += tokenizer.tokenize(comment)
    # ê¸¸ì´ê°€ 1ë³´ë‹¤ ê¸´ ë‹¨ì–´ë§Œ (ëª…ì‚¬ ì¤‘ì‹¬ ì¶”ì •)
    return [t for t in tokens if len(t) > 1]

# ------------------ Streamlit ì•± ------------------

st.title("ğŸ§  YouTube ëŒ“ê¸€ ëª…ì‚¬ ë¶„ì„ê¸°")

youtube_url = st.text_input("ğŸ“º YouTube ì˜ìƒ URL", value=SAMPLE_URL)

col1, col2 = st.columns(2)
with col1:
    select_count = st.selectbox("ëŒ“ê¸€ ê°œìˆ˜ (ë¹ ë¥¸ ì„ íƒ)", ["100", "500", "1000", "ëª¨ë‘"], index=0)
with col2:
    slider_count = st.slider("ëŒ“ê¸€ ê°œìˆ˜ (ì„¸ë¶€ ì¡°ì ˆ)", 100, 1000, step=100, value=100)

# ìˆ˜ì§‘ ìˆ˜ ê²°ì •
if select_count == "ëª¨ë‘":
    comment_limit = -1
else:
    comment_limit = max(int(select_count), slider_count)

if st.button("ë¶„ì„ ì‹œì‘"):
    video_id = extract_video_id(youtube_url)
    if not video_id:
        st.error("âš ï¸ ìœ íš¨í•œ YouTube URLì´ ì•„ë‹™ë‹ˆë‹¤.")
        st.stop()

    with st.spinner("ğŸ”„ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘..."):
        comments = get_comments(video_id, API_KEY, comment_limit)

    if not comments:
        st.warning("ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

    with st.spinner("ğŸ§  ëª…ì‚¬ ì¶”ì¶œ ì¤‘..."):
        nouns = extract_nouns(comments)
        freq = Counter(nouns)
        df_freq = pd.DataFrame(freq.items(), columns=["ë‹¨ì–´", "ë¹ˆë„ìˆ˜"]).sort_values(by="ë¹ˆë„ìˆ˜", ascending=False)

    st.subheader("ğŸ“Š ìƒìœ„ 20ê°œ ë‹¨ì–´ (ë¹ˆë„ìˆœ)")

    # ê¸°ë³¸ bar chart
    st.bar_chart(df_freq.head(20).set_index("ë‹¨ì–´"))

    # Altair bar chart
    st.altair_chart(
        alt.Chart(df_freq.head(20)).mark_bar().encode(
            x=alt.X("ë‹¨ì–´:N", sort="-y"),
            y="ë¹ˆë„ìˆ˜:Q",
            tooltip=["ë‹¨ì–´", "ë¹ˆë„ìˆ˜"]
        ).properties(
            width=600,
            height=400,
            title="ìƒìœ„ 20ê°œ ë‹¨ì–´ (Altair ì‹œê°í™”)"
        )
    )
