import streamlit as st
import pandas as pd
import requests
import io, os, tempfile, urllib.request
import matplotlib.pyplot as plt
from matplotlib import font_manager
from wordcloud import WordCloud
from collections import Counter
from googleapiclient.discovery import build
import re

# ğŸ”§ í°íŠ¸ ì„¤ì • í•¨ìˆ˜ (URL ìˆ˜ì •ë¨)
@st.cache_resource
def get_font_path():
    """ë‚˜ëˆ”ê³ ë”• í°íŠ¸ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # GitHub ì €ì¥ì†Œ êµ¬ì¡° ë³€ê²½ìœ¼ë¡œ ì¸í•´ URLì„ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.
    url = "https://raw.githubusercontent.com/google/fonts/main/ofl/nanumgothic/NanumGothic-Regular.ttf"
    tmp_path = os.path.join(tempfile.gettempdir(), "NanumGothic.ttf")
    if not os.path.exists(tmp_path):
        try:
            urllib.request.urlretrieve(url, tmp_path)
        except urllib.error.URLError as e:
            st.error(f"í°íŠ¸ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return None
    return tmp_path

# í°íŠ¸ ê²½ë¡œë¥¼ ê°€ì ¸ì˜¤ê³  Matplotlibì— ì„¤ì •í•©ë‹ˆë‹¤.
FONT_PATH = get_font_path()
if FONT_PATH:
    plt.rcParams["font.family"] = font_manager.FontProperties(fname=FONT_PATH).get_name()
else:
    st.error("í°íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ì–´ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")


# ğŸ“¦ ëŒ“ê¸€ ìˆ˜ì§‘ í•¨ìˆ˜
def get_comments(youtube_url, max_comments):
    """YouTube APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì§€ì •ëœ URLì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    try:
        video_id = youtube_url.split("v=")[-1].split("&")[0]
        # ì‚¬ìš©ìê°€ ì œê³µí•œ ì˜¬ë°”ë¥¸ API í‚¤ ì´ë¦„ìœ¼ë¡œ ìˆ˜ì •
        api_key = st.secrets["youtube_api_key"]
        youtube = build("youtube", "v3", developerKey=api_key)
        comments = []

        next_page_token = None
        while len(comments) < max_comments:
            # maxResultsëŠ” ìµœëŒ€ 100ì´ë¯€ë¡œ, ë‚¨ì€ ëŒ“ê¸€ ìˆ˜ì™€ 100 ì¤‘ ì‘ì€ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            request_count = min(100, max_comments - len(comments))
            if request_count <= 0:
                break

            response = youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                maxResults=request_count,
                pageToken=next_page_token,
                order="relevance",
                textFormat="plainText"
            ).execute()

            for item in response["items"]:
                comment = item["snippet"]["topLevelComment"]["snippet"]["textDisplay"]
                comments.append(comment)

            next_page_token = response.get("nextPageToken")
            if not next_page_token:
                break
        
        return comments

    except Exception as e:
        st.error(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.info("ì˜¬ë°”ë¥¸ YouTube ì˜ìƒ URLì¸ì§€, API í‚¤ê°€ ìœ íš¨í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return []

# ğŸ§¼ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
def clean_text(text):
    """íŠ¹ìˆ˜ë¬¸ì, ì´ëª¨í‹°ì½˜ ë“±ì„ ì œê±°í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•©ë‹ˆë‹¤."""
    # í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°±ë§Œ ë‚¨ê¸°ê³  ëª¨ë‘ ì œê±°
    cleaned_text = re.sub(r"[^\uAC00-\uD7A3a-zA-Z0-9\s]", "", text)
    return cleaned_text.strip()

def tokenize(texts, stopwords):
    """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë¶ˆìš©ì–´ë¥¼ ì œì™¸í•˜ê³  2ê¸€ì ì´ìƒì˜ í•œê¸€/ì˜ì–´ ë‹¨ì–´ë§Œ ì¶”ì¶œí•˜ì—¬ í† í°í™”í•©ë‹ˆë‹¤."""
    token_list = []
    for line in texts:
        # 2ê¸€ì ì´ìƒì˜ í•œê¸€ ë˜ëŠ” ì˜ì–´ ë‹¨ì–´ë§Œ ì¶”ì¶œ
        tokens = re.findall(r"[a-zA-Zê°€-í£]{2,}", line.lower()) # ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
        filtered_tokens = [word for word in tokens if word not in stopwords]
        token_list.extend(filtered_tokens)
    return token_list

# ğŸŒ¥ï¸ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± í•¨ìˆ˜
def generate_wordcloud(tokens, dpi=200, max_words=100):
    """ë‹¨ì–´ í† í°ì„ ê¸°ë°˜ìœ¼ë¡œ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ê³  Streamlitì— í‘œì‹œí•©ë‹ˆë‹¤."""
    if not FONT_PATH:
        st.error("í°íŠ¸ íŒŒì¼ì´ ì—†ì–´ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    word_freq = Counter(tokens).most_common(max_words)
    
    wc = WordCloud(
        font_path=FONT_PATH,
        background_color="white",
        width=800,
        height=600,
        max_words=max_words
    ).generate_from_frequencies(dict(word_freq))

    fig, ax = plt.subplots(figsize=(10, 7.5), dpi=dpi)
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    st.pyplot(fig)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Streamlit UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config("YouTube ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ", "â˜ï¸", layout="wide")
st.title("â˜ï¸ YouTube ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ê¸°")
st.markdown("YouTube ì˜ìƒì˜ ëŒ“ê¸€ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ ë‹¨ì–´ë¥¼ ë³´ì—¬ì£¼ëŠ” ì›Œë“œí´ë¼ìš°ë“œë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”!")

# ì‚¬ìš©ìê°€ ì œê³µí•œ ìƒ˜í”Œ URLì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
SAMPLE_URL = "https://www.youtube.com/watch?v=WXuK6gekU1Y"
youtube_url = st.text_input("ğŸ¥ YouTube ì˜ìƒ URL", value=SAMPLE_URL)

# st.expanderë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶ˆìš©ì–´ ì„¤ì • ë¶€ë¶„ì„ í† ê¸€ í˜•íƒœë¡œ ë³€ê²½
with st.expander("ğŸš« ë¶ˆìš©ì–´ ì„¤ì • (í´ë¦­í•˜ì—¬ ìˆ˜ì •)"):
    default_stopwords = "ã…‹ã…‹,ã…ã…,ã… ã… ,ì´,ê·¸,ì €,ê²ƒ,ìˆ˜,ë“±,ì¢€,ì˜,ë”,ì§„ì§œ,ë„ˆë¬´,ì™„ì „,ì •ë§,ê·¼ë°,ê·¸ë˜ì„œ,ê·¸ë¦¬ê³ ,í•˜ì§€ë§Œ,ì´ì œ,ì˜ìƒ,êµ¬ë…,ì¢‹ì•„ìš”,the,a,an,is,are,be,to,of,and,in,that,it,with,for,on,this,i,you,he,she,we,they,my,your,lol,omg,btw"
    user_stopwords = st.text_area(
        "ì œì™¸í•  ë‹¨ì–´ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
        value=default_stopwords,
        height=100,
        help="ë¶„ì„ì—ì„œ ì œì™¸í•˜ê³  ì‹¶ì€ ë‹¨ì–´ë¥¼ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•˜ì„¸ìš”."
    )

col1, col2 = st.columns(2)
with col1:
    max_comments = st.slider("ğŸ’¬ ë¶„ì„í•  ìµœëŒ€ ëŒ“ê¸€ ìˆ˜", min_value=100, max_value=2000, step=100, value=500)
with col2:
    max_words = st.slider("ğŸ”  ì›Œë“œí´ë¼ìš°ë“œì— í‘œì‹œí•  ë‹¨ì–´ ìˆ˜", min_value=20, max_value=200, step=10, value=100)

if st.button("ğŸš€ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"):
    if not youtube_url:
        st.warning("YouTube ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif not FONT_PATH:
        st.error("í°íŠ¸ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ì–´ ì•±ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¶ˆìš©ì–´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        stopword_list = [word.strip() for word in user_stopwords.lower().split(',') if word.strip()]
        
        with st.spinner("YouTube ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”..."):
            comments = get_comments(youtube_url, max_comments)

        if not comments:
            st.error("ëŒ“ê¸€ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜ìƒ ID, ëŒ“ê¸€ ê³µê°œ ì—¬ë¶€ ë˜ëŠ” API í‚¤ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            st.success(f"âœ… {len(comments)}ê°œì˜ ëŒ“ê¸€ì„ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")

            with st.spinner("í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•˜ê³  ë‹¨ì–´ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                cleaned = [clean_text(c) for c in comments]
                tokens = tokenize(cleaned, stopword_list)

            if not tokens:
                st.warning("ë¶„ì„í•  ìˆ˜ ìˆëŠ” ìœ íš¨í•œ ë‹¨ì–´(2ê¸€ì ì´ìƒ í•œê¸€/ì˜ì–´)ê°€ ëŒ“ê¸€ì— ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            else:
                st.info(f"ë¶„ì„ëœ ìœ íš¨ ë‹¨ì–´ ìˆ˜: {len(tokens)}ê°œ")
                with st.spinner("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    generate_wordcloud(tokens, dpi=300, max_words=max_words)
ã„¹
