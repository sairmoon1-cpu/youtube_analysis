import streamlit as st
import pandas as pd
import requests
import io, os, tempfile, urllib.request
import matplotlib.pyplot as plt
from matplotlib import font_manager
from wordcloud import WordCloud
from collections import Counter
from googleapiclient.discovery import build
import re

# ğŸ”§ í°íŠ¸ ì„¤ì • í•¨ìˆ˜
@st.cache_resource
def get_font_path():
    """ë‚˜ëˆ”ê³ ë”• í°íŠ¸ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    url = "https://raw.githubusercontent.com/google/fonts/main/ofl/nanumgothic/NanumGothic-Regular.ttf"
    tmp_path = os.path.join(tempfile.gettempdir(), "NanumGothic.ttf")
    if not os.path.exists(tmp_path):
        try:
            urllib.request.urlretrieve(url, tmp_path)
        except urllib.error.URLError as e:
            st.error(f"í°íŠ¸ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return None
    return tmp_path

# í°íŠ¸ ê²½ë¡œë¥¼ ê°€ì ¸ì˜¤ê³  Matplotlibì— ì„¤ì •í•©ë‹ˆë‹¤.
FONT_PATH = get_font_path()
if FONT_PATH:
    plt.rcParams["font.family"] = font_manager.FontProperties(fname=FONT_PATH).get_name()
else:
    st.error("í°íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ì–´ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")


# ğŸ“¦ ëŒ“ê¸€ ë° ì˜ìƒ ì œëª© ìˆ˜ì§‘ í•¨ìˆ˜
def get_video_data(youtube_url, max_comments):
    """YouTube APIë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ“ê¸€ê³¼ ì˜ìƒ ì œëª©ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    try:
        video_id = youtube_url.split("v=")[-1].split("&")[0]
        api_key = st.secrets["youtube_api_key"]
        youtube = build("youtube", "v3", developerKey=api_key)
        
        # ì˜ìƒ ì œëª© ê°€ì ¸ì˜¤ê¸°
        video_title = ""
        try:
            video_response = youtube.videos().list(
                part='snippet',
                id=video_id
            ).execute()
            video_title = video_response['items'][0]['snippet']['title']
        except Exception as e:
            st.warning(f"ì˜ìƒ ì œëª©ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
            video_title = "Untitled"


        # ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°
        comments = []
        next_page_token = None
        while len(comments) < max_comments:
            request_count = min(100, max_comments - len(comments))
            if request_count <= 0:
                break

            response = youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                maxResults=request_count,
                pageToken=next_page_token,
                order="relevance",
                textFormat="plainText"
            ).execute()

            for item in response["items"]:
                comment = item["snippet"]["topLevelComment"]["snippet"]["textDisplay"]
                comments.append(comment)

            next_page_token = response.get("nextPageToken")
            if not next_page_token:
                break
        
        return comments, video_title

    except Exception as e:
        st.error(f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.info("ì˜¬ë°”ë¥¸ YouTube ì˜ìƒ URLì¸ì§€, API í‚¤ê°€ ìœ íš¨í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return [], None

# ğŸ§¼ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
def clean_text(text):
    """íŠ¹ìˆ˜ë¬¸ì, ì´ëª¨í‹°ì½˜ ë“±ì„ ì œê±°í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•©ë‹ˆë‹¤."""
    cleaned_text = re.sub(r"[^\uAC00-\uD7A3a-zA-Z0-9\s]", "", text)
    return cleaned_text.strip()

def tokenize(texts, stopwords):
    """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë¶ˆìš©ì–´ë¥¼ ì œì™¸í•˜ê³  2ê¸€ì ì´ìƒì˜ í•œê¸€/ì˜ì–´ ë‹¨ì–´ë§Œ ì¶”ì¶œí•˜ì—¬ í† í°í™”í•©ë‹ˆë‹¤."""
    token_list = []
    for line in texts:
        tokens = re.findall(r"[a-zA-Zê°€-í£]{2,}", line.lower())
        filtered_tokens = [word for word in tokens if word not in stopwords]
        token_list.extend(filtered_tokens)
    return token_list

# ğŸŒ¥ï¸ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± í•¨ìˆ˜
def generate_wordcloud(tokens, dpi=200, max_words=100):
    """ë‹¨ì–´ í† í°ì„ ê¸°ë°˜ìœ¼ë¡œ ì›Œë“œí´ë¼ìš°ë“œ Figure ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not FONT_PATH:
        st.error("í°íŠ¸ íŒŒì¼ì´ ì—†ì–´ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    word_freq = Counter(tokens).most_common(max_words)
    
    wc = WordCloud(
        font_path=FONT_PATH,
        background_color="white",
        width=800,
        height=600,
        max_words=max_words
    ).generate_from_frequencies(dict(word_freq))

    fig, ax = plt.subplots(figsize=(10, 7.5), dpi=dpi)
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    return fig

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Streamlit UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config("YouTube ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ", "â˜ï¸", layout="wide")
st.title("â˜ï¸ YouTube ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ê¸°")
st.markdown("YouTube ì˜ìƒì˜ ëŒ“ê¸€ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ ë‹¨ì–´ë¥¼ ë³´ì—¬ì£¼ëŠ” ì›Œë“œí´ë¼ìš°ë“œë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”!")

SAMPLE_URL = "https://www.youtube.com/watch?v=WXuK6gekU1Y"
youtube_url = st.text_input("ğŸ¥ YouTube ì˜ìƒ URL", value=SAMPLE_URL)

with st.expander("ğŸš« ë¶ˆìš©ì–´ ì„¤ì • (í´ë¦­í•˜ì—¬ ìˆ˜ì •)"):
    default_stopwords = "ã…‹ã…‹,ã…ã…,ã… ã… ,ì´,ê·¸,ì €,ê²ƒ,ìˆ˜,ë“±,ì¢€,ì˜,ë”,ì§„ì§œ,ë„ˆë¬´,ì™„ì „,ì •ë§,ê·¼ë°,ê·¸ë˜ì„œ,ê·¸ë¦¬ê³ ,í•˜ì§€ë§Œ,ì´ì œ,ì˜ìƒ,êµ¬ë…,ì¢‹ì•„ìš”,the,a,an,is,are,be,to,of,and,in,that,it,with,for,on,this,i,you,he,she,we,they,my,your,lol,omg,btw"
    user_stopwords = st.text_area(
        "ì œì™¸í•  ë‹¨ì–´ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
        value=default_stopwords,
        height=100,
        help="ë¶„ì„ì—ì„œ ì œì™¸í•˜ê³  ì‹¶ì€ ë‹¨ì–´ë¥¼ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•˜ì„¸ìš”."
    )

col1, col2 = st.columns(2)
with col1:
    max_comments = st.slider("ğŸ’¬ ë¶„ì„í•  ìµœëŒ€ ëŒ“ê¸€ ìˆ˜", min_value=100, max_value=2000, step=100, value=500)
with col2:
    max_words = st.slider("ğŸ”  ì›Œë“œí´ë¼ìš°ë“œì— í‘œì‹œí•  ë‹¨ì–´ ìˆ˜", min_value=20, max_value=200, step=10, value=100)

if st.button("ğŸš€ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"):
    if not youtube_url:
        st.warning("YouTube ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif not FONT_PATH:
        st.error("í°íŠ¸ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ì–´ ì•±ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        stopword_list = [word.strip() for word in user_stopwords.lower().split(',') if word.strip()]
        
        with st.spinner("YouTube ëŒ“ê¸€ê³¼ ì˜ìƒ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            comments, video_title = get_video_data(youtube_url, max_comments)

        if not comments:
            st.error("ëŒ“ê¸€ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜ìƒ ID, ëŒ“ê¸€ ê³µê°œ ì—¬ë¶€ ë˜ëŠ” API í‚¤ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            st.success(f"âœ… '{video_title}' ì˜ìƒì˜ ëŒ“ê¸€ {len(comments)}ê°œë¥¼ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")

            with st.spinner("í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•˜ê³  ë‹¨ì–´ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                cleaned = [clean_text(c) for c in comments]
                tokens = tokenize(cleaned, stopword_list)

            if not tokens:
                st.warning("ë¶„ì„í•  ìˆ˜ ìˆëŠ” ìœ íš¨í•œ ë‹¨ì–´(2ê¸€ì ì´ìƒ í•œê¸€/ì˜ì–´)ê°€ ëŒ“ê¸€ì— ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            else:
                st.info(f"ë¶„ì„ëœ ìœ íš¨ ë‹¨ì–´ ìˆ˜: {len(tokens)}ê°œ")
                with st.spinner("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    wordcloud_fig = generate_wordcloud(tokens, dpi=200, max_words=max_words)
                    
                    if wordcloud_fig:
                        st.pyplot(wordcloud_fig)
                        
                        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ ì¶”ê°€
                        buf = io.BytesIO()
                        wordcloud_fig.savefig(buf, format="png", bbox_inches='tight')
                        
                        # íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°
                        clean_title = re.sub(r'[\\/*?:"<>|]', "", video_title)
                        file_name = f"{clean_title}_ì›Œë“œí´ë¼ìš°ë“œ.png"
                        
                        st.download_button(
                            label="ğŸ–¼ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ",
                            data=buf.getvalue(),
                            file_name=file_name,
                            mime="image/png"
                        )
