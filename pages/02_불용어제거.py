import streamlit as st
from googleapiclient.discovery import build
import pandas as pd
from collections import Counter
from soynlp.tokenizer import RegexTokenizer
import re
import altair as alt

# âœ… ìƒ˜í”Œ URL & API Key
SAMPLE_URL = "https://www.youtube.com/watch?v=WXuK6gekU1Y"
API_KEY = st.secrets["youtube_api_key"]

# ğŸ¯ video ID ì¶”ì¶œ
def extract_video_id(url):
    pattern = r"(?:v=|youtu\.be/)([\w-]+)"
    match = re.search(pattern, url)
    return match.group(1) if match else None

# ğŸ’¬ ëŒ“ê¸€ ìˆ˜ì§‘
def get_comments(video_id, api_key, max_comments=100):
    youtube = build("youtube", "v3", developerKey=api_key)
    comments = []
    next_page_token = None

    while True:
        response = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=100,
            pageToken=next_page_token,
            order="relevance",
            textFormat="plainText"
        ).execute()

        for item in response["items"]:
            snippet = item["snippet"]["topLevelComment"]["snippet"]
            comments.append(snippet["textDisplay"])

        next_page_token = response.get("nextPageToken")
        if not next_page_token or (max_comments != -1 and len(comments) >= max_comments):
            break

    return comments[:max_comments] if max_comments != -1 else comments

# ğŸš« í•œê¸€ + ì˜ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
DEFAULT_KO_STOPWORDS = set([
    "ì˜ìƒ", "ì •ë§", "ì§„ì§œ", "ë„ˆë¬´", "ê·¸ë¦¬ê³ ", "ì´ê±´", "í•´ì„œ", "í•˜ê²Œ", "í•˜ëŠ”", "ê²ƒ", "ë•Œë¬¸",
    "ë´¤ì–´ìš”", "ìˆì–´ìš”", "ì´ë ‡ê²Œ", "ê°™ì•„ìš”", "ì´ìš”", "ì…ë‹ˆë‹¤", "ê·¸ëƒ¥", "ìš°ë¦¬", "ì´ê²Œ", "ì €ëŠ”", "ê·¸ê±°"
])

DEFAULT_EN_STOPWORDS = set([
    "i", "me", "my", "myself", "we", "our", "ours", "ourselves",
    "you", "your", "yours", "yourself", "he", "him", "his",
    "she", "her", "it", "its", "they", "them", "their", "theirs",
    "what", "which", "who", "whom", "this", "that", "these", "those",
    "am", "is", "are", "was", "were", "be", "been", "being",
    "have", "has", "had", "do", "does", "did", "will", "would",
    "shall", "should", "can", "could", "a", "an", "the", "and",
    "but", "if", "or", "because", "as", "until", "while", "of",
    "at", "by", "for", "with", "about", "against", "between", "into",
    "through", "during", "before", "after", "above", "below", "to",
    "from", "up", "down", "in", "out", "on", "off", "over", "under",
    "again", "further", "then", "once", "here", "there", "why", "how",
    "all", "any", "both", "each", "few", "more", "most", "other",
    "some", "such", "no", "nor", "not", "only", "own", "same", "so",
    "than", "too", "very", "just"
])

# ğŸ§  ëª…ì‚¬ ê¸°ë°˜ í† í° ì¶”ì¶œ + ë¶ˆìš©ì–´ ì œê±°
@st.cache_data
def extract_meaningful_words(comments):
    tokenizer = RegexTokenizer()
    tokens = []
    for comment in comments:
        tokens += tokenizer.tokenize(comment.lower())  # ì†Œë¬¸ìí™” ì²˜ë¦¬

    # ë¶ˆìš©ì–´ ì œê±° (í•œê¸€/ì˜ì–´ ëª¨ë‘)
    tokens = [
        t for t in tokens
        if len(t) > 1 and t not in DEFAULT_KO_STOPWORDS and t not in DEFAULT_EN_STOPWORDS
    ]
    return tokens

# ------------------ Streamlit UI ------------------

st.title("ğŸ§  YouTube ëŒ“ê¸€ ëª…ì‚¬ ë¶„ì„ê¸° (í•œ/ì˜ ë¶ˆìš©ì–´ ì œê±°)")

youtube_url = st.text_input("ğŸ“º YouTube ì˜ìƒ URL", value=SAMPLE_URL)

col1, col2 = st.columns(2)
with col1:
    select_count = st.selectbox("ëŒ“ê¸€ ê°œìˆ˜ (ë¹ ë¥¸ ì„ íƒ)", ["100", "500", "1000", "ëª¨ë‘"], index=0)
with col2:
    slider_count = st.slider("ëŒ“ê¸€ ê°œìˆ˜ (ì„¸ë¶€ ì¡°ì ˆ)", 100, 1000, step=100, value=100)

comment_limit = -1 if select_count == "ëª¨ë‘" else max(int(select_count), slider_count)

if st.button("ë¶„ì„ ì‹œì‘"):
    video_id = extract_video_id(youtube_url)
    if not video_id:
        st.error("âš ï¸ ìœ íš¨í•œ YouTube URLì´ ì•„ë‹™ë‹ˆë‹¤.")
        st.stop()

    with st.spinner("ğŸ”„ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘..."):
        comments = get_comments(video_id, API_KEY, comment_limit)

    if not comments:
        st.warning("ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

    with st.spinner("ğŸ§  ëª…ì‚¬ ì¶”ì¶œ ë° ë¶ˆìš©ì–´ ì œê±° ì¤‘..."):
        clean_tokens = extract_meaningful_words(comments)
        freq = Counter(clean_tokens)
        df_freq = pd.DataFrame(freq.items(), columns=["ë‹¨ì–´", "ë¹ˆë„ìˆ˜"]).sort_values(by="ë¹ˆë„ìˆ˜", ascending=False)

    st.subheader("ğŸ“Š ìƒìœ„ 20ê°œ ë‹¨ì–´ (ë¶ˆìš©ì–´ ì œê±° í›„)")

    # st ê¸°ë³¸ bar_chart
    st.bar_chart(df_freq.head(20).set_index("ë‹¨ì–´"))

    # Altair ì°¨íŠ¸
    st.altair_chart(
        alt.Chart(df_freq.head(20)).mark_bar().encode(
            x=alt.X("ë‹¨ì–´:N", sort="-y"),
            y="ë¹ˆë„ìˆ˜:Q",
            tooltip=["ë‹¨ì–´", "ë¹ˆë„ìˆ˜"]
        ).properties(
            width=600,
            height=400,
            title="ìƒìœ„ 20ê°œ ë‹¨ì–´ (Altair ì‹œê°í™”)"
        )
    )
